{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Автор: Найденов Алексей\n",
    "\n",
    "# Задача\n",
    "Построить модель, предсказывающую категорию объявления по его заголовку, описанию и цене. Метрика для оценки качества - accuracy. Результат скоринга файла test.csv с помощью предложенного классификатора (csv-файл с двумя столбцами: item_id, category_id).\n",
    "Категории имеют иерархическую структуру, описанную в файле сategory.csv. Необходимо посчитать также accuracy модели на каждом уровне иерархии.\n",
    "\n",
    "# Описание\n",
    "Лучшее найденное решение довольно простое:\n",
    "\n",
    "## 1. Предобработка данных\n",
    "\n",
    "Поля `title` и `description`: удалить все знаки препинания, оставшиеся слова привести к нормальной форме (библиотека `pymorphy2`).\n",
    "\n",
    "## 2. Токенизация текста (+ TF-IDF)\n",
    "\n",
    "Использовать токены только из поля `title` (обучить на `title` и \"натравить\" на `title` + `description`). Неочевидное решение выбрано на основе того, что с большой долей вероятности значимые слова встречаются в заголовке, а описание помимо прочего содержит много \"мусора\".\n",
    "\n",
    "## 3. Классификация\n",
    "\n",
    "Лучше всего показал себя метод опорных векторов с линейным ядром (`LinerSVC`). Гиперпараметры подбирались в `GridSearch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# обработка текста\n",
    "import re\n",
    "import pymorphy2\n",
    "# работа с данными\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# анализ данных\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Данные\n",
    "Все данные хранятся в директории `data`, смежной с текущей (т.е. путь \"../data/\").\n",
    "\n",
    "Обучающий набор (\"train.csv\"):\n",
    "- `item_id` - ID объявления\n",
    "- `title` - заголовок объявления\n",
    "- `description` - описание объявления\n",
    "- `price` - цена\n",
    "- `category_id` - категория (зависимая переменная)\n",
    "\n",
    "Дополнительный набор, описывающий иерархию категорий:\n",
    "- `category_id` - ID категории\n",
    "- `name` - название категории, где через символ \"|\" перечислена вся иерархия категории (например, \"Бытовая электроника|Телефоны|Samsung\")\n",
    "\n",
    "\n",
    "## Замечание\n",
    "В случае если параметр `HAS_CLEAN` установлен в `True`, то мы пропускаем длительный этап очистки данных, и загружаем готовый файл непосредственно из директории `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HAS_CLEAN = True\n",
    "HAS_CLEAN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"CONFIG\"\"\"\n",
    "# директория с данными\n",
    "PATH_DATA = \"../data/\"\n",
    "# обучаемый набор\n",
    "PATH_TRAIN = PATH_DATA + (\"train.csv\" if not HAS_CLEAN else \"train_normal.csv\")\n",
    "# тестовый набор\n",
    "PATH_TEST = PATH_DATA + (\"test.csv\" if not HAS_CLEAN else \"test_normal.csv\")\n",
    "# категории\n",
    "PATH_CATS = PATH_DATA + \"category.csv\"\n",
    "# выходное поле\n",
    "target_label = u'category_id'\n",
    "# предикторы\n",
    "pred_labels = [u'title', u'description']\n",
    "title_label = u'title'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Чтение данных\"\"\"\n",
    "dtrain = pd.read_csv(PATH_TRAIN)\n",
    "dtest = pd.read_csv(PATH_TEST)\n",
    "cats = pd.read_csv(PATH_CATS) \n",
    "target = dtrain[target_label]\n",
    "title = dtrain[title_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Functions\"\"\"\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# для построенной на (x, y) модели возвращает accuracy на (x_test, y_test)\n",
    "def check_accuracy(clf, x, y, x_test, y_test):\n",
    "    clf.fit(x, y)\n",
    "    t = clf.predict(x_test)\n",
    "    return accuracy_score(y_test, t)\n",
    "\n",
    "# возвращает позицию n-го найденного символа char в строке line  \n",
    "def find_nth(line, char, n):\n",
    "    start = line.find(char)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = line.find(char, start+len(char))\n",
    "        n -= 1\n",
    "    return start\n",
    "\n",
    "# возвращает обрезанное имя категории, т.е.категорию уровня n \n",
    "def get_hierarchical_cat(cat=\"\", n=1):\n",
    "    line = cat.decode(\"utf-8\")\n",
    "    index = find_nth(line, \"|\", n)\n",
    "    return line[:index] if index > 0 else line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предобработка исходных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"CLEAN DATA\"\"\"\n",
    "pattern_not_word = re.compile(u'[^a-zA-Zа-яёЁА-Я0-9_]+')\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\"\"\"Удаление знаков препинания\"\"\"\n",
    "def filter_punc_mark(line):   \n",
    "    tokens = pattern_not_word.sub(' ', line).strip()\n",
    "    return tokens\n",
    "\"\"\"Получение нормальной формы слова (по-умолчанию первая форма слова)\"\"\"\n",
    "def normal_form(line):\n",
    "    return ' '.join([morph.parse(w)[0].normal_form for w in line.split()]) \n",
    "\n",
    "def get_concat_df(df, cols):\n",
    "    res = np.str(\"\")\n",
    "    for col in cols:\n",
    "        res += df[col] + np.str(\" \")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наборы данных в кодировке UTF-8 без BOM. Для обработки текста приходится декодировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Очитска трейна\"\"\"\n",
    "if not HAS_CLEAN:\n",
    "    for col in pred_labels:\n",
    "        dtrain[col] = dtrain[col].str.decode('utf-8').apply(filter_punc_mark).apply(normal_form).str.encode('utf-8')\n",
    "\n",
    "for col in pred_labels:\n",
    "    dtrain[col].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Очитска теста\"\"\"\n",
    "if not HAS_CLEAN:\n",
    "    for col in pred_labels:\n",
    "        dtest[col] = dtest[col].str.decode('utf-8').apply(filter_punc_mark).apply(normal_form).str.encode('utf-8')\n",
    "\n",
    "for col in pred_labels:\n",
    "    dtest[col].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Токенизация текста (+ TF-IDF)\n",
    "`TfidfVectorizer` - векторизирует и транфомирует текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Определили векторизатор\n",
    "vect = TfidfVectorizer(\n",
    "    analyzer='word', \n",
    "    lowercase=True,\n",
    "    ngram_range=(1,2),\n",
    "    token_pattern=u'(?u)\\\\b\\\\w+\\\\b',\n",
    "    binary='True',\n",
    "    min_df=1\n",
    ")\n",
    "# Обучили на `title`\n",
    "vect.fit(title)\n",
    "# Преобразовали конкатенированные предикторы на трейне (`title` и `description`)\n",
    "X_train = vect.transform(get_concat_df(dtrain, pred_labels))\n",
    "# Преобразовали конкатенированные предикторы на тесте (`title` и `description`)\n",
    "X_test = vect.transform(get_concat_df(dtest, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кроссвалидация\n",
    "\n",
    "Строим кроссвалидацию модели на всех уровнях иерархической структуры. Каждый уровень иерархии находим просто: обрезаем название категории по нужному по счету символу \"|\" (функция `get_hierarchical_cat`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"CrossValidation\"\"\"\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# Выполнение cv на каждом уровне иерархической структуры \n",
    "# (по-умолчанию для полной иерархии)\n",
    "def compute_hierarchical_cv(hierarchical_level=0):\n",
    "    if hierarchical_level > 0:\n",
    "        xtarget = pd.merge(\n",
    "            pd.DataFrame({'category_id': target}), \n",
    "            right=cats, \n",
    "            how='left', \n",
    "            on='category_id'\n",
    "        )['name'].apply(lambda x: get_hierarchical_cat(x, n=hierarchical_level))\n",
    "        print \"Hierarchy level: %d\" % hierarchical_level\n",
    "    else:\n",
    "        xtarget = target\n",
    "        print \"Full hierarchy\"\n",
    "    compute_cv(xtarget)\n",
    "    \n",
    "# Ввели свою cv так как специфично подготавливаем набор данных, \n",
    "# а свой Classifier писать не быстрее\n",
    "def compute_cv(target): \n",
    "    skf = StratifiedKFold(target, n_folds=3, random_state=42)\n",
    "    scores=[]\n",
    "    for train_index, test_index in skf:\n",
    "        # Определили векторизатор\n",
    "        vect = TfidfVectorizer(\n",
    "            analyzer='word', \n",
    "            lowercase=True,\n",
    "            ngram_range=(1,2),\n",
    "            token_pattern=u'(?u)\\\\b\\\\w+\\\\b',\n",
    "            binary='True',\n",
    "            min_df=1\n",
    "        )\n",
    "        # Обучили на `title`\n",
    "        vect.fit(title[train_index])\n",
    "        # Преобразовали конкатенированные предикторы на трейне (`title` и `description`)\n",
    "        X_train = vect.transform(get_concat_df(dtrain, pred_labels)[train_index])\n",
    "        # Преобразовали конкатенированные предикторы на тесте (`title` и `description`)\n",
    "        X_test = vect.transform(get_concat_df(dtrain, pred_labels)[test_index])\n",
    "        y_train, y_test = target[train_index], target[test_index]\n",
    "\n",
    "        clf = LinearSVC(\n",
    "            C=0.5,\n",
    "            tol=1e-05,\n",
    "            max_iter=100,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        scores.append(check_accuracy(clf, X_train, y_train, X_test, y_test))\n",
    "    print \"accuracy_score: %f\" % np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Глубина иерархической структуры = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full hierarchy\n",
      "accuracy_score: 0.888151\n",
      "Hierarchy level: 1\n",
      "accuracy_score: 0.960524\n",
      "Hierarchy level: 2\n",
      "accuracy_score: 0.945350\n",
      "Hierarchy level: 3\n",
      "accuracy_score: 0.891857\n",
      "Hierarchy level: 4\n",
      "accuracy_score: 0.888151\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    compute_hierarchical_cv(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказание категории на тестовом множестве"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LinearSVC(\n",
    "    C=0.5,\n",
    "    tol=1e-05,\n",
    "    max_iter=100\n",
    ")\n",
    "# Обучили на трейне\n",
    "clf.fit(X_train, target)\n",
    "# Предсказали для теста и записали в файл\n",
    "preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "        'item_id': dtest['item_id'],\n",
    "        'category_id': preds\n",
    "        }, columns=['item_id', 'category_id']).to_csv(\"Submit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Возможные направления для улучшения модели\n",
    "\n",
    "1. Изменить паттерн слова (`u'[^a-zA-Zа-яёЁА-Я0-9_]+'`) в функции удаления знаков препинания. Например, не убирать дефизы.\n",
    "2. Использовать цену в модели классификатора.\n",
    "3. Использовать стекинг:\n",
    "\n",
    " - Обучить новую модель на ответах-метках классификаторов первого уровня.\n",
    " - Обучить новую модель (XGBoost) на непрерывных коэффициентах для каждого класса при нескольких классификаторах. Итого будет 54\\*n непрерывных полей, где n - число классификаторов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
